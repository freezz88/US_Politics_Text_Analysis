{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9430fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\freez\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "'''import the needed libraries'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "# For tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "# For lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# For stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# For transforming SKLearn Coherence in Gensim Coherence\n",
    "import tmtoolkit\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "#Spellchecking\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb25f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  score  \\\n",
      "id                                                                 \n",
      "ov1ll3  A Right Wing Group in Texas Is Making up Fake ...    166   \n",
      "ouwc9i  DOJ sues Texas over Gov. Abbott’s order for la...     85   \n",
      "ouqkxi  From white evangelicals to QAnon believers, wh...     57   \n",
      "oun2lc  DeSantis says he’ll sign order allowing parent...    269   \n",
      "ouipnz  Show on the road: In Utah, Florida Gov. Ron De...     31   \n",
      "ou0w7e  Michigan Supreme Court limits use of restraint...    103   \n",
      "otzggh  'Election integrity committee' in York County ...    261   \n",
      "otzb3p  Texas Senator Used MLK’s Words To Attack Criti...    136   \n",
      "osvxyl  Wisconsin GOP leader doesn't want another elec...    105   \n",
      "osvwbq       Democrats press Biden to extend eviction ban     54   \n",
      "\n",
      "                                                      url  comms_num  \\\n",
      "id                                                                     \n",
      "ov1ll3  https://www.vice.com/en/article/wx5bg5/blm-whi...         34   \n",
      "ouwc9i  https://www.kxan.com/news/texas-politics/doj-s...         17   \n",
      "ouqkxi  https://www.modbee.com/news/coronavirus/articl...         27   \n",
      "oun2lc  https://www.orlandosentinel.com/politics/os-ne...        138   \n",
      "ouipnz  https://www.tallahassee.com/story/news/politic...         28   \n",
      "ou0w7e  https://www.metrotimes.com/news-hits/archives/...          2   \n",
      "otzggh  https://www.yorkdispatch.com/story/news/local/...         20   \n",
      "otzb3p  https://www.keranews.org/politics/2021-07-29/t...         18   \n",
      "osvxyl  https://abcnews.go.com/Politics/wireStory/wisc...         10   \n",
      "osvwbq  https://www.politico.com/news/2021/07/27/democ...         29   \n",
      "\n",
      "             created body            timestamp  \n",
      "id                                              \n",
      "ov1ll3  1.627710e+09  NaN  2021-07-31 08:35:47  \n",
      "ouwc9i  1.627688e+09  NaN  2021-07-31 02:26:12  \n",
      "ouqkxi  1.627671e+09  NaN  2021-07-30 21:45:09  \n",
      "oun2lc  1.627660e+09  NaN  2021-07-30 18:43:05  \n",
      "ouipnz  1.627644e+09  NaN  2021-07-30 14:21:54  \n",
      "ou0w7e  1.627576e+09  NaN  2021-07-29 19:30:52  \n",
      "otzggh  1.627572e+09  NaN  2021-07-29 18:16:28  \n",
      "otzb3p  1.627571e+09  NaN  2021-07-29 18:08:34  \n",
      "osvxyl  1.627422e+09  NaN  2021-07-28 00:43:18  \n",
      "osvwbq  1.627422e+09  NaN  2021-07-28 00:40:50  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      " \n",
      "Number of rows in DataFrame:  28063\n"
     ]
    }
   ],
   "source": [
    "'''Method for reading data from csv and save as type DataFrame (Pandas)'''\n",
    "'''index column in this special data set is the third column'''\n",
    "def inputData(url):\n",
    "    input_data_csv = pd.read_csv(url,index_col=2)\n",
    "    return input_data_csv\n",
    "    \n",
    "'''Declaration of variables'''\n",
    "'''Data Input as .csv from github'''\n",
    "'''@tfvectorizer: placeholder for Tfidf, that will be overwritten'''\n",
    "'''@lsamodel: placeholder for LSA, that will be overwritten'''\n",
    "'''@ldamodel: placeholder for lDA, that will be overwritten'''\n",
    "'''@chosenNumberTopics: number of topics used for the specific data set'''\n",
    "'''@data_url: URL for the input data from Githup Repository'''\n",
    "'''@param: ?raw=true in url important for using clean original data'''\n",
    "bow_vect = CountVectorizer()\n",
    "tfvectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)\n",
    "lsamodel = TruncatedSVD(n_components=10,algorithm='randomized',n_iter=10)\n",
    "ldamodel = LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1)\n",
    "chosenNumberTopics = 2\n",
    "data_url = 'https://github.com/freezz88/US_Politics_Text_Analysis/blob/main/reddit_politics.csv?raw=true'\n",
    "data = inputData(data_url)\n",
    "print(data.head(10))\n",
    "print(type(data))\n",
    "print(\" \")\n",
    "print(\"Number of rows in DataFrame: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc34b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in DataFrame after Cleaning:  17731\n",
      " \n",
      "Text after filtering the dataset:\n",
      "0        I had the same reasoning when I watch fox news...\n",
      "1             Unethical fucks will always find a loophole.\n",
      "2                                      Failed actual coup.\n",
      "3                   Why is trump even in the news anymore?\n",
      "4                   And it could be my head in a basket...\n",
      "                               ...                        \n",
      "17726            lil'wayne got a pardon and not them ah ah\n",
      "17727    So you think it will be called unconstitutiona...\n",
      "17728    The left of America has out numbered the right...\n",
      "17729    Everyone spread the word…I just set fire on water\n",
      "17730    Starting to feel like congress should let DOJ ...\n",
      "Name: body, Length: 17703, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text after text preprocessing:\n",
      "0         i had the same reasoning when i watch fox news..\n",
      "1             unethical fucks will always find a loophole.\n",
      "2                                      failed actual coup.\n",
      "3                   why is trump even in the news anymore?\n",
      "4                    and it could be my head in a basket..\n",
      "                               ...                        \n",
      "17698             lilwayne got a pardon and not them ah ah\n",
      "17699    so you think it will be called unconstitutional..\n",
      "17700    the left of america has out numbered the right ..\n",
      "17701     everyone spread the wordi just set fire on water\n",
      "17702    starting to feel like congress should let doj d..\n",
      "Length: 17703, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) \n",
    "    \n",
    "    # Remove special characters, keeping only words and basic charakters\n",
    "    # special for this data set: numbers are not interesting (only political buzzwords and temper)\n",
    "    # EXPERIMENTAL: Problems, if used with tokenization/stemming\n",
    "    #text = re.sub(r'[^a-zA-Z0-9\\s,.?!]', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z\\s,.?!]', '', text)  \n",
    "    \n",
    "    # Reduce massive character repetition to a maximum of two charakters\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)   \n",
    "    return text\n",
    "\n",
    "def appendIndividualStopwords(list):\n",
    "    for i in range(len(list)):\n",
    "        stopwords.append(list[i])\n",
    "\n",
    "'''Cleaning data'''\n",
    "# delete duplicate reviews - column body\n",
    "data.drop_duplicates(subset='body', inplace=True)\n",
    "# delete reviews without text\n",
    "data.dropna(subset=['body'], inplace=True)\n",
    "# Reset the index after the deletion of rows\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of rows in DataFrame after Cleaning: \",len(data))\n",
    "print(\" \")\n",
    "\n",
    "'''Filter text for the category comments'''\n",
    "'''Only show the column body, the others doesnt matter'''\n",
    "data_reviews = data['title'] == \"Comment\"\n",
    "filtered_data_list = data[data_reviews]\n",
    "reviews = filtered_data_list['body']\n",
    "# important: index = false removes the indexnumbers. Should not be visible in string representation.\n",
    "reviews_string = reviews.to_string(index=False)\n",
    "print(\"Text after filtering the dataset:\")\n",
    "print(reviews)\n",
    "print(type(reviews))\n",
    "print(\" \")\n",
    "\n",
    "'''Text Preprocessing'''\n",
    "\n",
    "'''I Download and definition of stopwords with NLTK'''\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "'''Define individual stopwords for data set'''\n",
    "list_indiv_stopwords = ['much', 'could', 'get', 'going', 'anything', 'something', 'someone', 'yes',\n",
    "                        'wasnt', 'since', 'still', 'means', 'hey', 'ah', 'thats', 'happen', 'no',\n",
    "                        'probably', 'ok', 'either', 'yo', 'basically', 'half', 'saw', 'also', 'aah',  \n",
    "                        'al', 'havent', 'didnt', 'there', 'maybe', 'im', 'nobody', 'st', 'wa', \n",
    "                        'nah', 'dont', 'youre', 'got', 'th', 'arent', 'would', 'ive', 'though', \n",
    "                        'isnt', 'ha', 'yep', 'shes', 'definitely', 'yeah', 'oh', 'hes', 'lot', 'id', 'else',\n",
    "                       'hi', 'wo', 'ye', 'ca', 'tha', 'thi', 'yup', 'nni', 'nn', 'su', 'hasnt', 'sh', 'ge', 'bc', \n",
    "                        'sur', 'theyre', 'gop', 'em', 'nnit', 'wi', 'theyll', 'whether', 'youve']\n",
    "#print(list_indiv_stopwords)\n",
    "appendIndividualStopwords(list_indiv_stopwords)\n",
    "#print(stopwords)\n",
    "#stop_words_english = set(stopwords.words('english'))\n",
    "\n",
    "'''I. Execution of text preprocessing'''\n",
    "changed_data = preprocess_text(reviews_string)\n",
    "\n",
    "'''II. Tokenization'''\n",
    "# Use the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Tokenize the text\n",
    "doc = nlp(changed_data)\n",
    "# Extract tokens\n",
    "tokens = [token.text for token in doc]\n",
    "#print(\"DataType tokens: \",type(tokens))\n",
    "#print(tokens)\n",
    "#print(\" \")\n",
    "\n",
    "# Using a spell checker to correct mistakes in the text\n",
    "# WARNING: Very long code execution times\n",
    "# NOT RECOMMENDED for this data set\n",
    "#spell = SpellChecker()\n",
    "#corrected_tokens = [spell.correction(token) if re.search(r'(.)\\1', token) else token for token in tokens]\n",
    "#print(corrected_tokens)\n",
    "\n",
    "'''III. Stemming / Lemmatization'''\n",
    "\n",
    "# For Stemming\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming each token\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "#print(\"Sentences after Stemming:\")\n",
    "#print(stemmed_tokens)\n",
    "#print(type(stemmed_tokens))\n",
    "#print(\" \")\n",
    "\n",
    "\n",
    "'''Convert string into a list. Split by lines.'''\n",
    "list_changed_data = changed_data.splitlines()   # important: data with good results\n",
    "#list_changed_data = stemmed_tokens # experimental: data after tokenization/stemming\n",
    "\n",
    "# converting list into series datatype\n",
    "preprocessed_data = pd.Series(list_changed_data)\n",
    "print(\"Text after text preprocessing:\")\n",
    "print(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c34fec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-Modell Daten\n",
      " \n",
      "       aapl  aardvark  ab  abandon  abb  abbot  abbott  abbotts  abboud  \\\n",
      "0         0         0   0        0    0      0       0        0       0   \n",
      "1         0         0   0        0    0      0       0        0       0   \n",
      "2         0         0   0        0    0      0       0        0       0   \n",
      "3         0         0   0        0    0      0       0        0       0   \n",
      "4         0         0   0        0    0      0       0        0       0   \n",
      "...     ...       ...  ..      ...  ...    ...     ...      ...     ...   \n",
      "17698     0         0   0        0    0      0       0        0       0   \n",
      "17699     0         0   0        0    0      0       0        0       0   \n",
      "17700     0         0   0        0    0      0       0        0       0   \n",
      "17701     0         0   0        0    0      0       0        0       0   \n",
      "17702     0         0   0        0    0      0       0        0       0   \n",
      "\n",
      "       abbreviation  ...  zeppelins  zero  zerodays  zerosum  zis  zodiac  \\\n",
      "0                 0  ...          0     0         0        0    0       0   \n",
      "1                 0  ...          0     0         0        0    0       0   \n",
      "2                 0  ...          0     0         0        0    0       0   \n",
      "3                 0  ...          0     0         0        0    0       0   \n",
      "4                 0  ...          0     0         0        0    0       0   \n",
      "...             ...  ...        ...   ...       ...      ...  ...     ...   \n",
      "17698             0  ...          0     0         0        0    0       0   \n",
      "17699             0  ...          0     0         0        0    0       0   \n",
      "17700             0  ...          0     0         0        0    0       0   \n",
      "17701             0  ...          0     0         0        0    0       0   \n",
      "17702             0  ...          0     0         0        0    0       0   \n",
      "\n",
      "       zombies  zoning  zuck  zuckerberg  \n",
      "0            0       0     0           0  \n",
      "1            0       0     0           0  \n",
      "2            0       0     0           0  \n",
      "3            0       0     0           0  \n",
      "4            0       0     0           0  \n",
      "...        ...     ...   ...         ...  \n",
      "17698        0       0     0           0  \n",
      "17699        0       0     0           0  \n",
      "17700        0       0     0           0  \n",
      "17701        0       0     0           0  \n",
      "17702        0       0     0           0  \n",
      "\n",
      "[17703 rows x 13241 columns]\n",
      " \n",
      "Höchste Wortvorkommen: \n",
      "dum           6\n",
      "blah          4\n",
      "huh           4\n",
      "people        3\n",
      "recall        3\n",
      "             ..\n",
      "focus         1\n",
      "focused       1\n",
      "focuses       1\n",
      "foe           1\n",
      "zuckerberg    1\n",
      "Length: 13241, dtype: int64\n",
      " \n"
     ]
    }
   ],
   "source": [
    "'''Implementation Bag-of-words'''\n",
    "def calculateBoW(showValues):\n",
    "    vect = CountVectorizer(stop_words=stopwords)\n",
    "    bow_data = vect.fit_transform(preprocessed_data)\n",
    "    bow_data = pd.DataFrame(bow_data.toarray(),columns=vect.get_feature_names())\n",
    "    '''Zwischenausgabe der Bow-Modell Daten'''\n",
    "    if (showValues):\n",
    "        print(\"BoW-Modell Daten\")\n",
    "        print(\" \")\n",
    "        print(bow_data)\n",
    "        print(\" \")\n",
    "        result = bow_data.max()\n",
    "        sorted_result = result.sort_values(ascending=False)\n",
    "        print(\"Höchste Wortvorkommen: \")\n",
    "        print(sorted_result)\n",
    "        print(\" \")\n",
    "    \n",
    "    \n",
    "calculateBoW(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf94fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-idf Daten Reviews\n",
      " \n",
      "       aapl  aardvark   ab  abandon  abb  abbot  abbott  abbotts  abboud  \\\n",
      "0       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "1       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "2       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "3       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "4       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "...     ...       ...  ...      ...  ...    ...     ...      ...     ...   \n",
      "17698   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17699   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17700   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17701   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17702   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "\n",
      "       abbreviation  ...  zeppelins  zero  zerodays  zerosum  zis  zodiac  \\\n",
      "0               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "1               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "2               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "3               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "4               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "...             ...  ...        ...   ...       ...      ...  ...     ...   \n",
      "17698           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17699           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17700           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17701           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17702           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "\n",
      "       zombies  zoning  zuck  zuckerberg  \n",
      "0          0.0     0.0   0.0         0.0  \n",
      "1          0.0     0.0   0.0         0.0  \n",
      "2          0.0     0.0   0.0         0.0  \n",
      "3          0.0     0.0   0.0         0.0  \n",
      "4          0.0     0.0   0.0         0.0  \n",
      "...        ...     ...   ...         ...  \n",
      "17698      0.0     0.0   0.0         0.0  \n",
      "17699      0.0     0.0   0.0         0.0  \n",
      "17700      0.0     0.0   0.0         0.0  \n",
      "17701      0.0     0.0   0.0         0.0  \n",
      "17702      0.0     0.0   0.0         0.0  \n",
      "\n",
      "[17703 rows x 13241 columns]\n",
      " \n",
      "Höchstes relatives Wortvorkommen: \n",
      "religious    1.000000\n",
      "brought      1.000000\n",
      "buddy        1.000000\n",
      "fascism      1.000000\n",
      "nominal      1.000000\n",
      "               ...   \n",
      "paulr        0.343976\n",
      "obey         0.331164\n",
      "romans       0.331164\n",
      "stud         0.306482\n",
      "immature     0.306482\n",
      "Length: 13241, dtype: float64\n",
      " \n"
     ]
    }
   ],
   "source": [
    "'''Implementation Tf-idf'''\n",
    "def calculateTfidf(showValues):\n",
    "    vectorizer = TfidfVectorizer(use_idf=True,\n",
    "    smooth_idf=True, stop_words=stopwords)\n",
    "    tfvectorizer = vectorizer\n",
    "    model = vectorizer.fit_transform(preprocessed_data)\n",
    "    data2=pd.DataFrame(model.toarray(),columns=vectorizer.get_feature_names())\n",
    "    '''Zwischenausgabe der TF-idf Daten'''\n",
    "    if (showValues):\n",
    "        print(\"TF-idf Daten Reviews\")\n",
    "        print(\" \")\n",
    "        print(data2)\n",
    "        print(\" \")\n",
    "        result = data2.max()\n",
    "        sorted_result = result.sort_values(ascending=False)\n",
    "        print(\"Höchstes relatives Wortvorkommen: \")\n",
    "        print(sorted_result)\n",
    "        print(\" \")\n",
    "    return model\n",
    "    \n",
    "tfvectorizer = calculateTfidf(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3c6d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateCoherenceScore(model, df_column):\n",
    "    topics = model.components_\n",
    "    n_best_words = 20\n",
    "    texts = [[word for word in doc.split()] for doc in df_column]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
    "\n",
    "    top_words = []\n",
    "    for topic in topics:\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-n_best_words - 1:-1]])\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=top_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "        \n",
    "def calculateLDA(model, tfvectorizer, topicNumber, showValues):\n",
    "    '''Implementierung der LDA-Technik der semantischen Analyse'''\n",
    "    lda_model=LatentDirichletAllocation(n_components=topicNumber,learning_method='online',random_state=42,max_iter=1)\n",
    "    lda_top=lda_model.fit_transform(model)\n",
    "    '''Zwischenausgabe der LDA Daten'''\n",
    "    if (showValues):\n",
    "        print(\"Latente Dirichlet Allocation LDA mit Themenanzahl \",topicNumber)\n",
    "        print(\" \")\n",
    "        print(\"Reviews: \")\n",
    "        for i,topic in enumerate(lda_top[0]):\n",
    "            print(\"Topic \",i,\" value \",\" : \",topic)\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        print(\"Documents by topic matrix: \",lda_top.shape)\n",
    "        print(\"Topic by word matrix: \",lda_model.components_.shape)\n",
    "        print(\" \")\n",
    "    return lda_model\n",
    "\n",
    "def calculateLSA(model, topicNumber, showValues):\n",
    "    '''Implementierung der LSA-Technik der semantischen Analyse'''\n",
    "    lsa_model = TruncatedSVD(n_components=topicNumber,algorithm='randomized',n_iter=10)\n",
    "    lsa = lsa_model.fit_transform(model)\n",
    "    lsa_first=lsa[0]\n",
    "    '''Zwischenausgabe der LSA Daten'''\n",
    "    if (showValues):\n",
    "        print(\"Latente semantische Analyse LSA mit Themenanzahl \",topicNumber)\n",
    "        print(\" \")\n",
    "        print(\"Reviews:\")\n",
    "        for i,topic in enumerate(lsa_first):\n",
    "            print(\"Topic \",i,\" value : \", topic)\n",
    "        print(\" \")\n",
    "    return lsa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ab7222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual coherence score:  0.6799307805252142 , number of topics:  2\n",
      "Highest previous coherence score:  0\n",
      "Information: Number of chosen topics was changed to  2  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.6895350952454397 , number of topics:  3\n",
      "Highest previous coherence score:  0.6799307805252142\n",
      "Information: Number of chosen topics was changed to  3  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.6841999598176893 , number of topics:  4\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6835625425493806 , number of topics:  5\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6768131830901768 , number of topics:  6\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6832144995424146 , number of topics:  7\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6816244701169709 , number of topics:  8\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6828782089378509 , number of topics:  9\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6771700464727901 , number of topics:  10\n",
      "Highest previous coherence score:  0.6895350952454397\n"
     ]
    }
   ],
   "source": [
    "def choseLDANumberTopicsByCoherenceScore():\n",
    "    previousCoherenceScore = [0]\n",
    "    for i in range(2,11):\n",
    "        model = calculateTfidf(False)\n",
    "        model = calculateLDA(model, tfvectorizer, i, False)\n",
    "        actualCoherence = calculateCoherenceScore(model,preprocessed_data)\n",
    "        print(\"Actual coherence score: \",actualCoherence,\", number of topics: \",i)\n",
    "        previousCoherenceScore.sort(reverse=True)\n",
    "        print(\"Highest previous coherence score: \",previousCoherenceScore[0])\n",
    "        \n",
    "        if (actualCoherence > previousCoherenceScore[0]):\n",
    "            chosenNumberTopics = i\n",
    "            print(\"Information: Number of chosen topics was changed to \",i, \" with better coherence score.\")\n",
    "            print(\" \")\n",
    "            \n",
    "        previousCoherenceScore.append(actualCoherence)\n",
    "    return chosenNumberTopics\n",
    "        \n",
    "chosenNumberTopics = choseLDANumberTopicsByCoherenceScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b0ea165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choseLSANumberTopicsByCoherenceScore():\n",
    "    previousCoherenceScore = [0]\n",
    "    for i in range(2,11):\n",
    "        model = calculateTfidf(False)\n",
    "        model = calculateLSA(model, i, False)\n",
    "        actualCoherence = calculateCoherenceScore(model,preprocessed_data)\n",
    "        print(\"Actual coherence score: \",actualCoherence,\", number of topics: \",i)\n",
    "        previousCoherenceScore.sort(reverse=True)\n",
    "        print(\"Highest previous coherence score: \",previousCoherenceScore[0])\n",
    "        \n",
    "        if (actualCoherence > previousCoherenceScore[0]):\n",
    "            chosenNumberTopics = i\n",
    "            print(\"Information: Number of chosen topics was changed to \",i, \" with better coherence score.\")\n",
    "            print(\" \")\n",
    "            \n",
    "        previousCoherenceScore.append(actualCoherence)\n",
    "    return chosenNumberTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74a1a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latente Dirichlet Allocation LDA mit Themenanzahl  3\n",
      " \n",
      "Reviews: \n",
      "Topic  0  value   :  0.11353413532940285\n",
      "Topic  1  value   :  0.7744273612958431\n",
      "Topic  2  value   :  0.11203850337475404\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 3)\n",
      "Topic by word matrix:  (3, 13241)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def calculateLDATextAnalysis(model, chosenNumberTopics):\n",
    "    model = calculateLDA(model, tfvectorizer, chosenNumberTopics, True)\n",
    "    return model\n",
    "    \n",
    "ldamodel = calculateLDATextAnalysis(tfvectorizer, chosenNumberTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bfb3848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual coherence score:  0.6823947282454832 , number of topics:  2\n",
      "Highest previous coherence score:  0\n",
      "Information: Number of chosen topics was changed to  2  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.6910735913642246 , number of topics:  3\n",
      "Highest previous coherence score:  0.6823947282454832\n",
      "Information: Number of chosen topics was changed to  3  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.6872746593861354 , number of topics:  4\n",
      "Highest previous coherence score:  0.6910735913642246\n",
      "Actual coherence score:  0.6836182868209463 , number of topics:  5\n",
      "Highest previous coherence score:  0.6910735913642246\n",
      "Actual coherence score:  0.6801413060521493 , number of topics:  6\n",
      "Highest previous coherence score:  0.6910735913642246\n",
      "Actual coherence score:  0.677831837446521 , number of topics:  7\n",
      "Highest previous coherence score:  0.6910735913642246\n",
      "Actual coherence score:  0.6771137786614844 , number of topics:  8\n",
      "Highest previous coherence score:  0.6910735913642246\n",
      "Actual coherence score:  0.6746937296815223 , number of topics:  9\n",
      "Highest previous coherence score:  0.6910735913642246\n",
      "Actual coherence score:  0.6716746337828134 , number of topics:  10\n",
      "Highest previous coherence score:  0.6910735913642246\n",
      "Latente semantische Analyse LSA mit Themenanzahl  3\n",
      " \n",
      "Reviews:\n",
      "Topic  0  value :  0.013722087826702908\n",
      "Topic  1  value :  -0.004335365439809157\n",
      "Topic  2  value :  0.004199852665496865\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def calculateLSATextAnalysis(model, chosenNumberTopics):\n",
    "    model = calculateLSA(model, chosenNumberTopics, True)\n",
    "    return model\n",
    "\n",
    "chosenNumberTopics = choseLSANumberTopicsByCoherenceScore()\n",
    "lsamodel = calculateLSATextAnalysis(tfvectorizer, chosenNumberTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c951818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Best words in topic for LDA model:\n",
      "Topic #0:\n",
      "think way said one never well law everyone absolutely trump literally every us better america read two even part republican tell says states made things great next getting democracy big done less use needs remember gonna yet americans government article makes dems live life help hillary sense please important evidence power forget sides laws clearly hard making nice thing everything\n",
      "Topic #1:\n",
      "people like know good really see point go lol republicans right sure already need doesnt shit exactly democrats thank first one true want mean guy saying state seems even left honestly understand talking well anyone thing real theres feel man trying fair vote stop president care look matter wouldnt least guns old god new hate coming reason voted conservative last\n",
      "Topic #2:\n",
      "make say trump actually biden cant work time wont nothing fuck years like agree court pay many bad believe pretty gun wrong ever mean right person money fucking lets far give country thought day problem enough voting keep guys okay wait love fact want almost texas whats abortion comment war best sounds us end long dude back idea white kind\n",
      " \n",
      "Best words in topic for LSA model:\n",
      "Topic #0:\n",
      "like people think know trump one really right say even good want mean well doesnt see republicans thing sure said biden sounds cant lol point make need saying nothing time way go never seems us vote many agree actually shit democrats believe theres literally feel exactly fuck already work looks give understand always bad things everyone care take stupid texas\n",
      "Topic #1:\n",
      "like sounds looks seems feel almost said one doesnt sound nothing yea act feels years word issues mean well reads look politicians article kind ago obama might acting setting wont read day law policies things needed life house week aoc girls dropped thank require lose share idiot fire based calling elected current pelosi full every says buy duck lives con\n",
      "Topic #2:\n",
      "think know point trump republicans right one good really thing agree way democrats biden mean lol anyone actually pretty first well person idea ever question guys stopped want theres done fair part far reason nice wrong important better honestly us personally disagree saying might needs less look time makes used thanks doesnt vote already sure trying missed left necessarily crazy\n"
     ]
    }
   ],
   "source": [
    "def printBestWordsInTopic(model, feature_names, n_best_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "            for i in topic.argsort()[:-n_best_words - 1:-1]])) \n",
    "\n",
    "def printNLPdata(ldamodel, vect):\n",
    "    n_top_words = 60\n",
    "\n",
    "    vect.fit_transform(preprocessed_data)\n",
    "    tf_feature_names = vect.get_feature_names()\n",
    "    print(\" \")\n",
    "    print(\"Best words in topic for LDA model:\")\n",
    "    printBestWordsInTopic(ldamodel, tf_feature_names, n_top_words)\n",
    "    \n",
    "    vect.fit_transform(preprocessed_data)\n",
    "    tf_feature_names = vect.get_feature_names()\n",
    "    print(\" \")\n",
    "    print(\"Best words in topic for LSA model:\")\n",
    "    printBestWordsInTopic(lsamodel, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "def plotNLPdata():\n",
    "    print(\"For future implementation\")\n",
    "    \n",
    "vect = CountVectorizer(stop_words=stopwords)\n",
    "printNLPdata(ldamodel, vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3413b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
