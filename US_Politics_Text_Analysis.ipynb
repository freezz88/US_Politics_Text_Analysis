{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9430fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries for this project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import gensim\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "# For tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# For lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# For stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For transforming SKLearn coherence in Gensim coherence\n",
    "import tmtoolkit\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
    "\n",
    "#Spellchecking\n",
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb25f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  score  \\\n",
      "id                                                                 \n",
      "ov1ll3  A Right Wing Group in Texas Is Making up Fake ...    166   \n",
      "ouwc9i  DOJ sues Texas over Gov. Abbott’s order for la...     85   \n",
      "ouqkxi  From white evangelicals to QAnon believers, wh...     57   \n",
      "oun2lc  DeSantis says he’ll sign order allowing parent...    269   \n",
      "ouipnz  Show on the road: In Utah, Florida Gov. Ron De...     31   \n",
      "ou0w7e  Michigan Supreme Court limits use of restraint...    103   \n",
      "otzggh  'Election integrity committee' in York County ...    261   \n",
      "otzb3p  Texas Senator Used MLK’s Words To Attack Criti...    136   \n",
      "osvxyl  Wisconsin GOP leader doesn't want another elec...    105   \n",
      "osvwbq       Democrats press Biden to extend eviction ban     54   \n",
      "\n",
      "                                                      url  comms_num  \\\n",
      "id                                                                     \n",
      "ov1ll3  https://www.vice.com/en/article/wx5bg5/blm-whi...         34   \n",
      "ouwc9i  https://www.kxan.com/news/texas-politics/doj-s...         17   \n",
      "ouqkxi  https://www.modbee.com/news/coronavirus/articl...         27   \n",
      "oun2lc  https://www.orlandosentinel.com/politics/os-ne...        138   \n",
      "ouipnz  https://www.tallahassee.com/story/news/politic...         28   \n",
      "ou0w7e  https://www.metrotimes.com/news-hits/archives/...          2   \n",
      "otzggh  https://www.yorkdispatch.com/story/news/local/...         20   \n",
      "otzb3p  https://www.keranews.org/politics/2021-07-29/t...         18   \n",
      "osvxyl  https://abcnews.go.com/Politics/wireStory/wisc...         10   \n",
      "osvwbq  https://www.politico.com/news/2021/07/27/democ...         29   \n",
      "\n",
      "             created body            timestamp  \n",
      "id                                              \n",
      "ov1ll3  1.627710e+09  NaN  2021-07-31 08:35:47  \n",
      "ouwc9i  1.627688e+09  NaN  2021-07-31 02:26:12  \n",
      "ouqkxi  1.627671e+09  NaN  2021-07-30 21:45:09  \n",
      "oun2lc  1.627660e+09  NaN  2021-07-30 18:43:05  \n",
      "ouipnz  1.627644e+09  NaN  2021-07-30 14:21:54  \n",
      "ou0w7e  1.627576e+09  NaN  2021-07-29 19:30:52  \n",
      "otzggh  1.627572e+09  NaN  2021-07-29 18:16:28  \n",
      "otzb3p  1.627571e+09  NaN  2021-07-29 18:08:34  \n",
      "osvxyl  1.627422e+09  NaN  2021-07-28 00:43:18  \n",
      "osvwbq  1.627422e+09  NaN  2021-07-28 00:40:50  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      " \n",
      "Number of rows in DataFrame:  28063\n"
     ]
    }
   ],
   "source": [
    "def input_data(url):\n",
    "    '''Method for reading data from csv and save as type DataFrame (Pandas).\n",
    "    Index column in this special data set is the third column.\n",
    "    Exception Handling: FileNotFoundError.\n",
    "    '''\n",
    "    try:\n",
    "        input_data_csv = pd.read_csv(url,index_col=2)\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found!\")\n",
    "        return\n",
    "    #input_data_csv = pd.read_csv(url,index_col=2)\n",
    "    return input_data_csv\n",
    "    \n",
    "# Declaration of variables'''\n",
    "# Data Input as .csv from github'''\n",
    "# 'tfvectorizer': placeholder for Tfidf, that will be overwritten'''\n",
    "# 'lsamodel': placeholder for LSA, that will be overwritten'''\n",
    "# 'ldamodel': placeholder for lDA, that will be overwritten'''\n",
    "# 'chosen_number_topics': number of topics used for the specific data set'''\n",
    "# 'data_url': URL for the input data from Githup Repository'''\n",
    "# @param: ?raw=true in url important for using clean original data'''\n",
    "bow_vect = CountVectorizer()\n",
    "tfvectorizer = TfidfVectorizer(use_idf=True, smooth_idf=True)\n",
    "lsamodel = TruncatedSVD(n_components=10,algorithm='randomized',n_iter=10)\n",
    "ldamodel = LatentDirichletAllocation(n_components=10,learning_method='online',random_state=42,max_iter=1)\n",
    "chosen_number_topics = 2\n",
    "data_url = 'https://github.com/freezz88/US_Politics_Text_Analysis/blob/main/reddit_politics.csv?raw=true'\n",
    "data = input_data(data_url)\n",
    "\n",
    "print(data.head(10))\n",
    "print(type(data))\n",
    "print(\" \")\n",
    "print(\"Number of rows in DataFrame: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc34b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in DataFrame after Cleaning:  17731\n",
      " \n",
      "Text after filtering the dataset:\n",
      "0        I had the same reasoning when I watch fox news...\n",
      "1             Unethical fucks will always find a loophole.\n",
      "2                                      Failed actual coup.\n",
      "3                   Why is trump even in the news anymore?\n",
      "4                   And it could be my head in a basket...\n",
      "                               ...                        \n",
      "17726            lil'wayne got a pardon and not them ah ah\n",
      "17727    So you think it will be called unconstitutiona...\n",
      "17728    The left of America has out numbered the right...\n",
      "17729    Everyone spread the word…I just set fire on water\n",
      "17730    Starting to feel like congress should let DOJ ...\n",
      "Name: body, Length: 17703, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      " \n",
      "Text after text preprocessing:\n",
      "0         i had the same reasoning when i watch fox news..\n",
      "1             unethical fucks will always find a loophole.\n",
      "2                                      failed actual coup.\n",
      "3                   why is trump even in the news anymore?\n",
      "4                    and it could be my head in a basket..\n",
      "                               ...                        \n",
      "17698             lilwayne got a pardon and not them ah ah\n",
      "17699    so you think it will be called unconstitutional..\n",
      "17700    the left of america has out numbered the right ..\n",
      "17701     everyone spread the wordi just set fire on water\n",
      "17702    starting to feel like congress should let doj d..\n",
      "Length: 17703, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    '''Text Preprocessing for 'text'.\n",
    "    Different functions used to get a better text for text analysis.\n",
    "    '''\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text) \n",
    "    \n",
    "    # Remove special characters, keeping only words and basic charakters\n",
    "    #\n",
    "    # Special for this data set: numbers are not interesting\n",
    "    # (only political buzzwords and temper will be relevant)\n",
    "    # Problems occour, if used before tokenization/stemming\n",
    "    #text = re.sub(r'[^a-zA-Z0-9\\s,.?!]', '', text)  \n",
    "    text = re.sub(r'[^a-zA-Z\\s,.?!]', '', text)  \n",
    "    \n",
    "    # Reduce massive character repetition to a maximum of two charakters\n",
    "    # Important for this special data set: much common speech on Reddit\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)   \n",
    "    return text\n",
    "\n",
    "def append_individual_stopwords(list):\n",
    "    for i in range(len(list)):\n",
    "        stopwords.append(list[i])\n",
    "\n",
    "# Cleaning data\n",
    "# Delete duplicate reviews - column body\n",
    "data.drop_duplicates(subset='body', inplace=True)\n",
    "\n",
    "# Delete reviews without text\n",
    "data.dropna(subset=['body'], inplace=True)\n",
    "\n",
    "# Reset the index after the deletion of rows\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of rows in DataFrame after Cleaning: \",len(data))\n",
    "print(\" \")\n",
    "\n",
    "# Filter text for the text-category comments\n",
    "# Only show the column body, the others are not interesting for analysis\n",
    "data_reviews = data['title'] == \"Comment\"\n",
    "filtered_data_list = data[data_reviews]\n",
    "reviews = filtered_data_list['body']\n",
    "\n",
    "# @param: 'index=False' removes the indexnumbers. Should not be visible.\n",
    "reviews_string = reviews.to_string(index=False)\n",
    "print(\"Text after filtering the dataset:\")\n",
    "print(reviews)\n",
    "print(type(reviews))\n",
    "print(\" \")\n",
    "\n",
    "# Text Preprocessing\n",
    "#\n",
    "# I. Download and definition of stopwords with NLTK\n",
    "nltk.download(\"stopwords\")\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "'''Define individual stopwords for data set'''\n",
    "list_indiv_stopwords = ['much', 'could', 'get', 'going', \n",
    "                        'anything', 'something', 'someone', 'yes',\n",
    "                        'wasnt', 'since', 'still', 'means', 'hey', \n",
    "                        'ah', 'thats', 'happen', 'no',\n",
    "                        'probably', 'ok', 'either', 'yo', 'basically', \n",
    "                        'half', 'saw', 'also', 'aah',  \n",
    "                        'al', 'havent', 'didnt', 'there', 'maybe', \n",
    "                        'im', 'nobody', 'st', 'wa', \n",
    "                        'nah', 'dont', 'youre', 'got', 'th', 'arent', \n",
    "                        'would', 'ive', 'though', \n",
    "                        'isnt', 'ha', 'yep', 'shes', 'definitely', \n",
    "                        'yeah', 'oh', 'hes', 'lot', 'id', 'else',\n",
    "                        'hi', 'wo', 'ye', 'ca', 'tha', 'thi', 'yup', \n",
    "                        'nni', 'nn', 'su', 'hasnt', 'sh', 'ge', 'bc', \n",
    "                        'sur', 'theyre', 'gop', 'em', 'nnit', 'wi', \n",
    "                        'theyll', 'whether', 'youve']\n",
    "\n",
    "append_individual_stopwords(list_indiv_stopwords)\n",
    "\n",
    "# II. Execution of text preprocessing\n",
    "changed_data = preprocess_text(reviews_string)\n",
    "\n",
    "# III. Tokenization\n",
    "# Deactivated, because of bad results in topic modelling.\n",
    "# For future implementations and other NLP projects\n",
    "# Use the spaCy model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "#\n",
    "# Tokenize the text\n",
    "# doc = nlp(changed_data)\n",
    "#\n",
    "# Extract tokens\n",
    "# tokens = [token.text for token in doc]\n",
    "#\n",
    "# print(\"DataType tokens: \",type(tokens))\n",
    "# print(tokens)\n",
    "# print(\" \")\n",
    "#\n",
    "# Using a spell checker to correct mistakes in the text\n",
    "# WARNING: Very long code execution times\n",
    "# NOT RECOMMENDED for this data set\n",
    "#spell = SpellChecker()\n",
    "#corrected_tokens = [spell.correction(token) \n",
    "#                        if re.search(r'(.)\\1', token) else token for token in tokens]\n",
    "#print(corrected_tokens)\n",
    "\n",
    "# IV. Stemming / Lemmatization\n",
    "# Deactivated, because of bad results in topic modelling.\n",
    "# For future implementations and other NLP projects\n",
    "#\n",
    "# Initialize the stemmer\n",
    "# stemmer = PorterStemmer()\n",
    "#\n",
    "# Stemming each token\n",
    "# stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "#\n",
    "#print(\"Sentences after Stemming:\")\n",
    "#print(stemmed_tokens)\n",
    "#print(type(stemmed_tokens))\n",
    "#print(\" \")\n",
    "\n",
    "\n",
    "# Convert string into a list. Split by lines.\n",
    "list_changed_data = changed_data.splitlines()   # Important: data with good results\n",
    "# list_changed_data = stemmed_tokens # Not recommended: data after tokenization/stemming\n",
    "\n",
    "# Converting list into series datatype\n",
    "preprocessed_data = pd.Series(list_changed_data)\n",
    "print(\"Text after text preprocessing:\")\n",
    "print(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c34fec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-Modell Daten\n",
      " \n",
      "       aapl  aardvark  ab  abandon  abb  abbot  abbott  abbotts  abboud  \\\n",
      "0         0         0   0        0    0      0       0        0       0   \n",
      "1         0         0   0        0    0      0       0        0       0   \n",
      "2         0         0   0        0    0      0       0        0       0   \n",
      "3         0         0   0        0    0      0       0        0       0   \n",
      "4         0         0   0        0    0      0       0        0       0   \n",
      "...     ...       ...  ..      ...  ...    ...     ...      ...     ...   \n",
      "17698     0         0   0        0    0      0       0        0       0   \n",
      "17699     0         0   0        0    0      0       0        0       0   \n",
      "17700     0         0   0        0    0      0       0        0       0   \n",
      "17701     0         0   0        0    0      0       0        0       0   \n",
      "17702     0         0   0        0    0      0       0        0       0   \n",
      "\n",
      "       abbreviation  ...  zeppelins  zero  zerodays  zerosum  zis  zodiac  \\\n",
      "0                 0  ...          0     0         0        0    0       0   \n",
      "1                 0  ...          0     0         0        0    0       0   \n",
      "2                 0  ...          0     0         0        0    0       0   \n",
      "3                 0  ...          0     0         0        0    0       0   \n",
      "4                 0  ...          0     0         0        0    0       0   \n",
      "...             ...  ...        ...   ...       ...      ...  ...     ...   \n",
      "17698             0  ...          0     0         0        0    0       0   \n",
      "17699             0  ...          0     0         0        0    0       0   \n",
      "17700             0  ...          0     0         0        0    0       0   \n",
      "17701             0  ...          0     0         0        0    0       0   \n",
      "17702             0  ...          0     0         0        0    0       0   \n",
      "\n",
      "       zombies  zoning  zuck  zuckerberg  \n",
      "0            0       0     0           0  \n",
      "1            0       0     0           0  \n",
      "2            0       0     0           0  \n",
      "3            0       0     0           0  \n",
      "4            0       0     0           0  \n",
      "...        ...     ...   ...         ...  \n",
      "17698        0       0     0           0  \n",
      "17699        0       0     0           0  \n",
      "17700        0       0     0           0  \n",
      "17701        0       0     0           0  \n",
      "17702        0       0     0           0  \n",
      "\n",
      "[17703 rows x 13241 columns]\n",
      " \n",
      "Höchste Wortvorkommen: \n",
      "dum           6\n",
      "blah          4\n",
      "huh           4\n",
      "people        3\n",
      "recall        3\n",
      "             ..\n",
      "focus         1\n",
      "focused       1\n",
      "focuses       1\n",
      "foe           1\n",
      "zuckerberg    1\n",
      "Length: 13241, dtype: int64\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def calculate_bow(showValues):\n",
    "    '''Implementation Bag-of-words.\n",
    "    Counts the absolute number of each word.\n",
    "    If 'show_Values' is 'True', than print results. \n",
    "    '''\n",
    "    vect = CountVectorizer(stop_words=stopwords)\n",
    "    bow_data = vect.fit_transform(preprocessed_data)\n",
    "    bow_data = pd.DataFrame(bow_data.toarray(),columns=vect.get_feature_names())\n",
    "\n",
    "    if (showValues):\n",
    "        print(\"BoW-Modell Daten\")\n",
    "        print(\" \")\n",
    "        print(bow_data)\n",
    "        print(\" \")\n",
    "        result = bow_data.max()\n",
    "        sorted_result = result.sort_values(ascending=False)\n",
    "        print(\"Höchste Wortvorkommen: \")\n",
    "        print(sorted_result)\n",
    "        print(\" \")\n",
    "    \n",
    "    \n",
    "calculate_bow(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf94fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-idf Daten Reviews\n",
      " \n",
      "       aapl  aardvark   ab  abandon  abb  abbot  abbott  abbotts  abboud  \\\n",
      "0       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "1       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "2       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "3       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "4       0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "...     ...       ...  ...      ...  ...    ...     ...      ...     ...   \n",
      "17698   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17699   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17700   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17701   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "17702   0.0       0.0  0.0      0.0  0.0    0.0     0.0      0.0     0.0   \n",
      "\n",
      "       abbreviation  ...  zeppelins  zero  zerodays  zerosum  zis  zodiac  \\\n",
      "0               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "1               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "2               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "3               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "4               0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "...             ...  ...        ...   ...       ...      ...  ...     ...   \n",
      "17698           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17699           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17700           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17701           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "17702           0.0  ...        0.0   0.0       0.0      0.0  0.0     0.0   \n",
      "\n",
      "       zombies  zoning  zuck  zuckerberg  \n",
      "0          0.0     0.0   0.0         0.0  \n",
      "1          0.0     0.0   0.0         0.0  \n",
      "2          0.0     0.0   0.0         0.0  \n",
      "3          0.0     0.0   0.0         0.0  \n",
      "4          0.0     0.0   0.0         0.0  \n",
      "...        ...     ...   ...         ...  \n",
      "17698      0.0     0.0   0.0         0.0  \n",
      "17699      0.0     0.0   0.0         0.0  \n",
      "17700      0.0     0.0   0.0         0.0  \n",
      "17701      0.0     0.0   0.0         0.0  \n",
      "17702      0.0     0.0   0.0         0.0  \n",
      "\n",
      "[17703 rows x 13241 columns]\n",
      " \n",
      "Höchstes relatives Wortvorkommen: \n",
      "religious    1.000000\n",
      "brought      1.000000\n",
      "buddy        1.000000\n",
      "fascism      1.000000\n",
      "nominal      1.000000\n",
      "               ...   \n",
      "paulr        0.343976\n",
      "obey         0.331164\n",
      "romans       0.331164\n",
      "stud         0.306482\n",
      "immature     0.306482\n",
      "Length: 13241, dtype: float64\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def calculate_tfidf(showValues):\n",
    "    '''Implementation Tf-idf.\n",
    "    Counts the relative number of each word in documents.\n",
    "    If 'show_Values' is 'True', than print results. \n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(use_idf=True,\n",
    "    smooth_idf=True, stop_words=stopwords)\n",
    "    tfvectorizer = vectorizer\n",
    "    model = vectorizer.fit_transform(preprocessed_data)\n",
    "    data2=pd.DataFrame(model.toarray(),columns=vectorizer.get_feature_names())\n",
    "\n",
    "    if (showValues):\n",
    "        print(\"TF-idf Daten Reviews\")\n",
    "        print(\" \")\n",
    "        print(data2)\n",
    "        print(\" \")\n",
    "        result = data2.max()\n",
    "        sorted_result = result.sort_values(ascending=False)\n",
    "        print(\"Höchstes relatives Wortvorkommen: \")\n",
    "        print(sorted_result)\n",
    "        print(\" \")\n",
    "    return model\n",
    "    \n",
    "tfvectorizer = calculate_tfidf(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c6d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherence_score(model, df_column):\n",
    "    '''Calculate coherence score for a specific model.\n",
    "    Variable 'model' is the used model.\n",
    "    Variable 'df_columns' contains the data in one column.\n",
    "    Datatype 'Series' from Pandas is recommended.\n",
    "    '''\n",
    "    topics = model.components_\n",
    "    n_best_words = 20\n",
    "    texts = [[word for word in doc.split()] for doc in df_column]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    feature_names = [dictionary[i] for i in range(len(dictionary))]\n",
    "\n",
    "    top_words = []\n",
    "    for topic in topics:\n",
    "        top_words.append([feature_names[i] for i in topic.argsort()[:-n_best_words - 1:-1]])\n",
    "\n",
    "    coherence_model = CoherenceModel(topics=top_words, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model.get_coherence()\n",
    "    return coherence_score\n",
    "        \n",
    "def calculate_LDA(model, tfvectorizer, topicNumber, showValues):\n",
    "    '''Calculate LDA as a NLP semantic analysis method.\n",
    "    Variable 'model' is the used model.\n",
    "    Variable 'tfvectorizer' contains the vectorized data.\n",
    "    Variable 'topicNumber' defines the number of topics for topic modelling.\n",
    "    If 'showValues' is 'True', than print the results.\n",
    "    '''\n",
    "    lda_model=LatentDirichletAllocation(n_components=topicNumber,learning_method='online',random_state=42,max_iter=1)\n",
    "    lda_top=lda_model.fit_transform(model)\n",
    "\n",
    "    if (showValues):\n",
    "        print(\"Latente Dirichlet Allocation LDA mit Themenanzahl \",topicNumber)\n",
    "        print(\" \")\n",
    "        print(\"Reviews: \")\n",
    "        for i,topic in enumerate(lda_top[0]):\n",
    "            print(\"Topic \",i,\" value \",\" : \",topic)\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        print(\"Documents by topic matrix: \",lda_top.shape)\n",
    "        print(\"Topic by word matrix: \",lda_model.components_.shape)\n",
    "        print(\" \")\n",
    "    return lda_model\n",
    "\n",
    "def calculate_LSA(model, topicNumber, showValues):\n",
    "    '''Calculate LSA as a NLP semantic analysis method.\n",
    "    Variable 'model' is the used model.\n",
    "    Variable 'topicNumber' defines the number of topics for topic modelling.\n",
    "    If 'showValues' is 'True', than print the results.\n",
    "    '''\n",
    "    lsa_model = TruncatedSVD(n_components=topicNumber,algorithm='randomized',n_iter=10)\n",
    "    lsa = lsa_model.fit_transform(model)\n",
    "    lsa_first=lsa[0]\n",
    "\n",
    "    if (showValues):\n",
    "        print(\"Latente semantische Analyse LSA mit Themenanzahl \",topicNumber)\n",
    "        print(\" \")\n",
    "        print(\"Reviews:\")\n",
    "        for i,topic in enumerate(lsa_first):\n",
    "            print(\"Topic \",i,\" value : \", topic)\n",
    "        print(\" \")\n",
    "    return lsa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2ab7222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual coherence score:  0.6799307805252142 , number of topics:  2\n",
      "Highest previous coherence score:  0\n",
      "Information: Number of chosen topics was changed to  2  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.6895350952454397 , number of topics:  3\n",
      "Highest previous coherence score:  0.6799307805252142\n",
      "Information: Number of chosen topics was changed to  3  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.6841999598176893 , number of topics:  4\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6835625425493806 , number of topics:  5\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6768131830901768 , number of topics:  6\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6832144995424146 , number of topics:  7\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6816244701169709 , number of topics:  8\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6828782089378509 , number of topics:  9\n",
      "Highest previous coherence score:  0.6895350952454397\n",
      "Actual coherence score:  0.6771700464727901 , number of topics:  10\n",
      "Highest previous coherence score:  0.6895350952454397\n"
     ]
    }
   ],
   "source": [
    "def choose_LDA_number_topics_by_coherence():\n",
    "    '''Calculate best coherence score for LDA NLP method.\n",
    "    Calculations for topic numbers from two to ten.\n",
    "    Returns the number of optimal topics by coherence score.\n",
    "    '''\n",
    "    previous_coherence_score = [0]\n",
    "    for i in range(2,11):\n",
    "        model = calculate_tfidf(False)\n",
    "        model = calculate_LDA(model, tfvectorizer, i, False)\n",
    "        actual_coherence = calculate_coherence_score(model,preprocessed_data)\n",
    "        print(\"Actual coherence score: \",actual_coherence,\", number of topics: \",i)\n",
    "        previous_coherence_score.sort(reverse=True)\n",
    "        print(\"Highest previous coherence score: \",previous_coherence_score[0])\n",
    "        \n",
    "        if (actual_coherence > previous_coherence_score[0]):\n",
    "            chosen_number_topics = i\n",
    "            print(\"Information: Number of chosen topics was changed to \",i, \" with better coherence score.\")\n",
    "            print(\" \")\n",
    "            \n",
    "        previous_coherence_score.append(actual_coherence)\n",
    "    return chosen_number_topics\n",
    "        \n",
    "chosen_number_topics = choose_LDA_number_topics_by_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b0ea165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_LSA_number_topics_by_coherence():\n",
    "    '''Calculate best coherence score for LSA NLP method.\n",
    "    Calculations for topic numbers from two to ten.\n",
    "    Returns the number of optimal topics by coherence score.\n",
    "    '''\n",
    "    previous_coherence_score = [0]\n",
    "    for i in range(2,11):\n",
    "        model = calculate_tfidf(False)\n",
    "        model = calculate_LSA(model, i, False)\n",
    "        actual_coherence = calculate_coherence_score(model,preprocessed_data)\n",
    "        print(\"Actual coherence score: \",actual_coherence,\", number of topics: \",i)\n",
    "        previous_coherence_score.sort(reverse=True)\n",
    "        print(\"Highest previous coherence score: \",previous_coherence_score[0])\n",
    "        \n",
    "        if (actual_coherence > previous_coherence_score[0]):\n",
    "            chosen_number_topics = i\n",
    "            print(\"Information: Number of chosen topics was changed to \",i, \" with better coherence score.\")\n",
    "            print(\" \")\n",
    "            \n",
    "        previous_coherence_score.append(actual_coherence)\n",
    "    return chosen_number_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a1a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latente Dirichlet Allocation LDA mit Themenanzahl  3\n",
      " \n",
      "Reviews: \n",
      "Topic  0  value   :  0.11353413532940285\n",
      "Topic  1  value   :  0.7744273612958431\n",
      "Topic  2  value   :  0.11203850337475404\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 3)\n",
      "Topic by word matrix:  (3, 13241)\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def calculate_LDA_text_analysis(model, chosen_number_topics):\n",
    "    '''Calculate LDA NLP method.\n",
    "    Variable 'model' is the used model.\n",
    "    Variable 'chosen_number_topics' is the number of topics. \n",
    "    Returns the new model as the result.\n",
    "    '''\n",
    "    model = calculate_LDA(model, tfvectorizer, chosen_number_topics, True)\n",
    "    return model\n",
    "\n",
    "# Use the calculated optimal number of topics by coherence score for LDA method\n",
    "ldamodel = calculate_LDA_text_analysis(tfvectorizer, chosen_number_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bfb3848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual coherence score:  0.6823947282454831 , number of topics:  2\n",
      "Highest previous coherence score:  0\n",
      "Information: Number of chosen topics was changed to  2  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.6913999974444884 , number of topics:  3\n",
      "Highest previous coherence score:  0.6823947282454831\n",
      "Information: Number of chosen topics was changed to  3  with better coherence score.\n",
      " \n",
      "Actual coherence score:  0.685995840632831 , number of topics:  4\n",
      "Highest previous coherence score:  0.6913999974444884\n",
      "Actual coherence score:  0.6844454981754314 , number of topics:  5\n",
      "Highest previous coherence score:  0.6913999974444884\n",
      "Actual coherence score:  0.6810472091424837 , number of topics:  6\n",
      "Highest previous coherence score:  0.6913999974444884\n",
      "Actual coherence score:  0.6771010838732042 , number of topics:  7\n",
      "Highest previous coherence score:  0.6913999974444884\n",
      "Actual coherence score:  0.6753882306066455 , number of topics:  8\n",
      "Highest previous coherence score:  0.6913999974444884\n",
      "Actual coherence score:  0.6749724472967035 , number of topics:  9\n",
      "Highest previous coherence score:  0.6913999974444884\n",
      "Actual coherence score:  0.6786863004335089 , number of topics:  10\n",
      "Highest previous coherence score:  0.6913999974444884\n",
      "Latente semantische Analyse LSA mit Themenanzahl  3\n",
      " \n",
      "Reviews:\n",
      "Topic  0  value :  0.01372234311585612\n",
      "Topic  1  value :  -0.0043381322364590994\n",
      "Topic  2  value :  0.004212709923712711\n",
      " \n"
     ]
    }
   ],
   "source": [
    "def calculate_LSA_text_analysis(model, chosen_number_topics):\n",
    "    '''Calculate LSA NLP method.\n",
    "    Variable 'model' is the used model.\n",
    "    Variable 'chosen_number_topics' is the number of topics. \n",
    "    Returns the new model as the result.\n",
    "    '''\n",
    "    model = calculate_LSA(model, chosen_number_topics, True)\n",
    "    return model\n",
    "\n",
    "# The number of optimal topics had to be calculated again for LSA method\n",
    "chosen_number_topics = choose_LSA_number_topics_by_coherence()\n",
    "lsamodel = calculate_LSA_text_analysis(tfvectorizer, chosen_number_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c951818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Best words in topic for LDA model:\n",
      "Topic #0:\n",
      "think way said one never well law everyone absolutely trump literally every us better america read two even part republican tell says states made things great next getting democracy big done less use needs remember gonna yet americans government article makes dems live life help hillary sense please important evidence power forget sides laws clearly hard making nice thing everything\n",
      "Topic #1:\n",
      "people like know good really see point go lol republicans right sure already need doesnt shit exactly democrats thank first one true want mean guy saying state seems even left honestly understand talking well anyone thing real theres feel man trying fair vote stop president care look matter wouldnt least guns old god new hate coming reason voted conservative last\n",
      "Topic #2:\n",
      "make say trump actually biden cant work time wont nothing fuck years like agree court pay many bad believe pretty gun wrong ever mean right person money fucking lets far give country thought day problem enough voting keep guys okay wait love fact want almost texas whats abortion comment war best sounds us end long dude back idea white kind\n",
      " \n",
      "Best words in topic for LSA model:\n",
      "Topic #0:\n",
      "like people think know trump one really right say even good want mean well doesnt see republicans thing sure said biden sounds cant lol point make need saying nothing time way go never seems us vote many agree actually shit democrats believe theres literally feel exactly fuck already work looks give understand always bad things everyone care take stupid texas\n",
      "Topic #1:\n",
      "like sounds looks seems feel almost said one sound doesnt nothing yea act feels years word issues well reads look mean politicians article kind ago obama might acting setting wont read day policies law things needed life house week aoc girls dropped thank require lose share idiot fire based calling elected current pelosi full every says buy duck lives con\n",
      "Topic #2:\n",
      "think know point trump republicans right one good really thing agree way democrats lol biden mean anyone actually pretty person first idea well question ever guys stopped want theres done part fair far reason nice better wrong important honestly us personally disagree needs saying might look less makes used thanks time sure vote doesnt trying already missed left fucking necessarily\n"
     ]
    }
   ],
   "source": [
    "def print_best_words_in_topic(model, feature_names, n_best_words):\n",
    "    '''Prints the words with highest values in all topics.\n",
    "    Representation as a list of words divided in topics.\n",
    "    Variable 'model' is the used model.\n",
    "    Variable 'feature_names' are the names of features.\n",
    "    Variable 'n_best_words' is the number of words for each topic printed.\n",
    "    '''\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "            for i in topic.argsort()[:-n_best_words - 1:-1]])) \n",
    "\n",
    "def print_NLP_data(ldamodel, vect):\n",
    "    '''Prints the results of LDA and LSA methods.\n",
    "    Representation as two lists of words divided in topics for both methods.\n",
    "    '''\n",
    "    n_top_words = 60\n",
    "\n",
    "    vect.fit_transform(preprocessed_data)\n",
    "    tf_feature_names = vect.get_feature_names()\n",
    "    print(\" \")\n",
    "    print(\"Best words in topic for LDA model:\")\n",
    "    print_best_words_in_topic(ldamodel, tf_feature_names, n_top_words)\n",
    "    \n",
    "    vect.fit_transform(preprocessed_data)\n",
    "    tf_feature_names = vect.get_feature_names()\n",
    "    print(\" \")\n",
    "    print(\"Best words in topic for LSA model:\")\n",
    "    print_best_words_in_topic(lsamodel, tf_feature_names, n_top_words)\n",
    "\n",
    "\n",
    "def plot_NLP_data():\n",
    "    '''Not used: Future implementation for matplotlib.'''\n",
    "    print(\"For future implementation\")\n",
    "    \n",
    "# Printing the results of this NLP project for the specific data set    \n",
    "vect = CountVectorizer(stop_words=stopwords)\n",
    "print_NLP_data(ldamodel, vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc3413b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
