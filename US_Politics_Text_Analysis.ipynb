{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9430fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "'''import the needed libraries'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "# For tokenization - ToDo maybe not necessary\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "# For lemmatization - ToDo maybe not necessary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# For stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# For transforming SKLearn Coherence in Gensim Coherence\n",
    "import tmtoolkit\n",
    "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb25f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  score  \\\n",
      "id                                                                 \n",
      "ov1ll3  A Right Wing Group in Texas Is Making up Fake ...    166   \n",
      "ouwc9i  DOJ sues Texas over Gov. Abbott’s order for la...     85   \n",
      "ouqkxi  From white evangelicals to QAnon believers, wh...     57   \n",
      "oun2lc  DeSantis says he’ll sign order allowing parent...    269   \n",
      "ouipnz  Show on the road: In Utah, Florida Gov. Ron De...     31   \n",
      "ou0w7e  Michigan Supreme Court limits use of restraint...    103   \n",
      "otzggh  'Election integrity committee' in York County ...    261   \n",
      "otzb3p  Texas Senator Used MLK’s Words To Attack Criti...    136   \n",
      "osvxyl  Wisconsin GOP leader doesn't want another elec...    105   \n",
      "osvwbq       Democrats press Biden to extend eviction ban     54   \n",
      "\n",
      "                                                      url  comms_num  \\\n",
      "id                                                                     \n",
      "ov1ll3  https://www.vice.com/en/article/wx5bg5/blm-whi...         34   \n",
      "ouwc9i  https://www.kxan.com/news/texas-politics/doj-s...         17   \n",
      "ouqkxi  https://www.modbee.com/news/coronavirus/articl...         27   \n",
      "oun2lc  https://www.orlandosentinel.com/politics/os-ne...        138   \n",
      "ouipnz  https://www.tallahassee.com/story/news/politic...         28   \n",
      "ou0w7e  https://www.metrotimes.com/news-hits/archives/...          2   \n",
      "otzggh  https://www.yorkdispatch.com/story/news/local/...         20   \n",
      "otzb3p  https://www.keranews.org/politics/2021-07-29/t...         18   \n",
      "osvxyl  https://abcnews.go.com/Politics/wireStory/wisc...         10   \n",
      "osvwbq  https://www.politico.com/news/2021/07/27/democ...         29   \n",
      "\n",
      "             created body            timestamp  \n",
      "id                                              \n",
      "ov1ll3  1.627710e+09  NaN  2021-07-31 08:35:47  \n",
      "ouwc9i  1.627688e+09  NaN  2021-07-31 02:26:12  \n",
      "ouqkxi  1.627671e+09  NaN  2021-07-30 21:45:09  \n",
      "oun2lc  1.627660e+09  NaN  2021-07-30 18:43:05  \n",
      "ouipnz  1.627644e+09  NaN  2021-07-30 14:21:54  \n",
      "ou0w7e  1.627576e+09  NaN  2021-07-29 19:30:52  \n",
      "otzggh  1.627572e+09  NaN  2021-07-29 18:16:28  \n",
      "otzb3p  1.627571e+09  NaN  2021-07-29 18:08:34  \n",
      "osvxyl  1.627422e+09  NaN  2021-07-28 00:43:18  \n",
      "osvwbq  1.627422e+09  NaN  2021-07-28 00:40:50  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Number of rows in DataFrame:  28063\n"
     ]
    }
   ],
   "source": [
    "'''Method for reading data from csv and save as type DataFrame (Pandas)'''\n",
    "'''index column in this special data set is the third column'''\n",
    "def inputData(url):\n",
    "    input_data_csv = pd.read_csv(url,index_col=2)\n",
    "    return input_data_csv\n",
    "    \n",
    "'''Declaration of variables'''\n",
    "'''Data Input as .csv from github'''\n",
    "'''@model: placeholder, that will be overwritten'''\n",
    "'''@param: ?raw=true in url important for using clean original data'''\n",
    "model = TruncatedSVD(n_components=10,algorithm='randomized',n_iter=10)\n",
    "data_url = 'https://github.com/freezz88/US_Politics_Text_Analysis/blob/main/reddit_politics.csv?raw=true'\n",
    "data = inputData(data_url)\n",
    "print(data.head(10))\n",
    "print(type(data))\n",
    "print(\"Number of rows in DataFrame: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc34b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in DataFrame after Cleaning:  17731\n",
      "Text after filtering the dataset:\n",
      "0        I had the same reasoning when I watch fox news...\n",
      "1             Unethical fucks will always find a loophole.\n",
      "2                                      Failed actual coup.\n",
      "3                   Why is trump even in the news anymore?\n",
      "4                   And it could be my head in a basket...\n",
      "                               ...                        \n",
      "17726            lil'wayne got a pardon and not them ah ah\n",
      "17727    So you think it will be called unconstitutiona...\n",
      "17728    The left of America has out numbered the right...\n",
      "17729    Everyone spread the word…I just set fire on water\n",
      "17730    Starting to feel like congress should let DOJ ...\n",
      "Name: body, Length: 17703, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      " \n",
      "Text after text preprocessing:\n",
      "0         i had the same reasoning when i watch fox news..\n",
      "1             unethical fucks will always find a loophole.\n",
      "2                                      failed actual coup.\n",
      "3                   why is trump even in the news anymore?\n",
      "4                    and it could be my head in a basket..\n",
      "                               ...                        \n",
      "17698             lilwayne got a pardon and not them ah ah\n",
      "17699    so you think it will be called unconstitutional..\n",
      "17700    the left of america has out numbered the right ..\n",
      "17701     everyone spread the wordi just set fire on water\n",
      "17702    starting to feel like congress should let doj d..\n",
      "Length: 17703, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Cleaning data'''\n",
    "# delete duplicate reviews - column body\n",
    "data.drop_duplicates(subset='body', inplace=True)\n",
    "# delete reviews without text\n",
    "data.dropna(subset=['body'], inplace=True)\n",
    "# Reset the index after the deletion of rows\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of rows in DataFrame after Cleaning: \",len(data))\n",
    "\n",
    "'''Filter text for the category comments'''\n",
    "'''Only show the column body, the others doesnt matter'''\n",
    "data_reviews = data['title'] == \"Comment\"\n",
    "filtered_data_list = data[data_reviews]\n",
    "reviews = filtered_data_list['body']\n",
    "# important: index = false removes the indexnumbers. Should not be visible in string representation.\n",
    "reviews_string = reviews.to_string(index=False)\n",
    "print(\"Text after filtering the dataset:\")\n",
    "print(reviews)\n",
    "print(type(reviews))\n",
    "print(\" \")\n",
    "\n",
    "'''Text Preprocessing'''\n",
    "'''I Tokenization - ToDo check later for optimization '''\n",
    "\n",
    "'''II Download and definition of stopwords with NLTK - ToDo append new stopwords'''\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "'''III Stemming / Lemmatization - ToDo check later for optimization'''\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  \n",
    "    # Remove special characters, keeping only words and basic charakters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s,.?!]', '', text)  \n",
    "    # Reduce massive character repetition to a maximum of two charakters\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)   \n",
    "    return text\n",
    "\n",
    "'''Execution of text preprocessing'''\n",
    "changed_data = preprocess_text(reviews_string)\n",
    "\n",
    "'''Convert string into a list. Split by lines.'''\n",
    "list_changed_data = changed_data.splitlines()\n",
    "\n",
    "# Use the spaCy model\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Tokenize the text\n",
    "#doc = nlp(list_changed_data)\n",
    "# Extract tokens\n",
    "#tokens = [token.text for token in doc]\n",
    "#print(\"DataType tokens: \",type(tokens))\n",
    "#print(tokens)\n",
    "#print(\" \")\n",
    "\n",
    "# For Stemming\n",
    "# Initialize the stemmer\n",
    "#stemmer = PorterStemmer()\n",
    "# Stemming each token\n",
    "#stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "#print(\"Sentences after Stemming:\")\n",
    "#print(stemmed_tokens)\n",
    "#print(type(stemmed_tokens))\n",
    "#print(\" \")\n",
    "\n",
    "# converting list into series datatype\n",
    "preprocessed_data = pd.Series(list_changed_data)\n",
    "print(\"Text after text preprocessing:\")\n",
    "print(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c34fec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-Modell Daten\n",
      " \n",
      "       00  001share  05  08  10  100  100b  100k  100nni  100s  ...  \\\n",
      "0       0         0   0   0   0    0     0     0       0     0  ...   \n",
      "1       0         0   0   0   0    0     0     0       0     0  ...   \n",
      "2       0         0   0   0   0    0     0     0       0     0  ...   \n",
      "3       0         0   0   0   0    0     0     0       0     0  ...   \n",
      "4       0         0   0   0   0    0     0     0       0     0  ...   \n",
      "...    ..       ...  ..  ..  ..  ...   ...   ...     ...   ...  ...   \n",
      "17698   0         0   0   0   0    0     0     0       0     0  ...   \n",
      "17699   0         0   0   0   0    0     0     0       0     0  ...   \n",
      "17700   0         0   0   0   0    0     0     0       0     0  ...   \n",
      "17701   0         0   0   0   0    0     0     0       0     0  ...   \n",
      "17702   0         0   0   0   0    0     0     0       0     0  ...   \n",
      "\n",
      "       zeppelins  zero  zerodays  zerosum  zis  zodiac  zombies  zoning  zuck  \\\n",
      "0              0     0         0        0    0       0        0       0     0   \n",
      "1              0     0         0        0    0       0        0       0     0   \n",
      "2              0     0         0        0    0       0        0       0     0   \n",
      "3              0     0         0        0    0       0        0       0     0   \n",
      "4              0     0         0        0    0       0        0       0     0   \n",
      "...          ...   ...       ...      ...  ...     ...      ...     ...   ...   \n",
      "17698          0     0         0        0    0       0        0       0     0   \n",
      "17699          0     0         0        0    0       0        0       0     0   \n",
      "17700          0     0         0        0    0       0        0       0     0   \n",
      "17701          0     0         0        0    0       0        0       0     0   \n",
      "17702          0     0         0        0    0       0        0       0     0   \n",
      "\n",
      "       zuckerberg  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "...           ...  \n",
      "17698           0  \n",
      "17699           0  \n",
      "17700           0  \n",
      "17701           0  \n",
      "17702           0  \n",
      "\n",
      "[17703 rows x 13600 columns]\n",
      " \n",
      "Höchste Wortvorkommen: \n",
      "00            4\n",
      "001share      1\n",
      "05            1\n",
      "08            1\n",
      "10            1\n",
      "             ..\n",
      "zodiac        1\n",
      "zombies       1\n",
      "zoning        1\n",
      "zuck          1\n",
      "zuckerberg    1\n",
      "Length: 13600, dtype: int64\n",
      " \n"
     ]
    }
   ],
   "source": [
    "'''Implementation Bag-of-words'''\n",
    "def calculateBoW():\n",
    "    vect = CountVectorizer(stop_words=stop_words_english)\n",
    "    bow_data = vect.fit_transform(preprocessed_data)\n",
    "    bow_data = pd.DataFrame(bow_data.toarray(),columns=vect.get_feature_names())\n",
    "    '''Zwischenausgabe der Bow-Modell Daten'''\n",
    "    print(\"BoW-Modell Daten\")\n",
    "    print(\" \")\n",
    "    print(bow_data)\n",
    "    print(\" \")\n",
    "    print(\"Höchste Wortvorkommen: \")\n",
    "    print(bow_data.max())\n",
    "    print(\" \")\n",
    "    \n",
    "    \n",
    "calculateBoW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf94fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-idf Daten Reviews\n",
      " \n",
      "        00  001share   05   08   10  100  100b  100k  100nni  100s  ...  \\\n",
      "0      0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "1      0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "2      0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "3      0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "4      0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "...    ...       ...  ...  ...  ...  ...   ...   ...     ...   ...  ...   \n",
      "17698  0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "17699  0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "17700  0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "17701  0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "17702  0.0       0.0  0.0  0.0  0.0  0.0   0.0   0.0     0.0   0.0  ...   \n",
      "\n",
      "       zeppelins  zero  zerodays  zerosum  zis  zodiac  zombies  zoning  zuck  \\\n",
      "0            0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "1            0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "2            0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "3            0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "4            0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "...          ...   ...       ...      ...  ...     ...      ...     ...   ...   \n",
      "17698        0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "17699        0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "17700        0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "17701        0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "17702        0.0   0.0       0.0      0.0  0.0     0.0      0.0     0.0   0.0   \n",
      "\n",
      "       zuckerberg  \n",
      "0             0.0  \n",
      "1             0.0  \n",
      "2             0.0  \n",
      "3             0.0  \n",
      "4             0.0  \n",
      "...           ...  \n",
      "17698         0.0  \n",
      "17699         0.0  \n",
      "17700         0.0  \n",
      "17701         0.0  \n",
      "17702         0.0  \n",
      "\n",
      "[17703 rows x 13600 columns]\n",
      " \n",
      "Höchstes relatives Wortvorkommen: \n",
      "00            0.910114\n",
      "001share      0.536688\n",
      "05            0.501657\n",
      "08            0.585216\n",
      "10            0.695003\n",
      "                ...   \n",
      "zodiac        0.522834\n",
      "zombies       0.438811\n",
      "zoning        0.527926\n",
      "zuck          0.603261\n",
      "zuckerberg    0.603699\n",
      "Length: 13600, dtype: float64\n",
      " \n"
     ]
    }
   ],
   "source": [
    "'''Implementation Tf-idf'''\n",
    "'''ToDo wrong data - the data from text preprocessing not used'''\n",
    "def calculateTfidf():\n",
    "    #vectorizer = TfidfVectorizer(min_df=1) first version TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(use_idf=True,\n",
    "    smooth_idf=True, stop_words=stop_words_english)\n",
    "    #model = vectorizer.fit_transform([reviews_string]) wrong usage, wrong datatype\n",
    "    model = vectorizer.fit_transform(preprocessed_data)\n",
    "    data2=pd.DataFrame(model.toarray(),columns=vectorizer.get_feature_names())\n",
    "    '''Zwischenausgabe der TF-idf Daten'''\n",
    "    print(\"TF-idf Daten Reviews\")\n",
    "    print(\" \")\n",
    "    print(data2)\n",
    "    print(\" \")\n",
    "    print(\"Höchstes relatives Wortvorkommen: \")\n",
    "    print(data2.max())\n",
    "    print(\" \")\n",
    "    return model\n",
    "    \n",
    "model = calculateTfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e076930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latente semantische Analyse LSA mit Themenanzahl  2\n",
      " \n",
      "Reviews:\n",
      "Topic  0  :  0.014615463172164502\n",
      "Topic  1  :  -0.006171938490511813\n",
      " \n",
      "Latente semantische Analyse LSA mit Themenanzahl  3\n",
      " \n",
      "Reviews:\n",
      "Topic  0  :  0.014615268617039252\n",
      "Topic  1  :  -0.006184413661743188\n",
      "Topic  2  :  -0.0011356260091640146\n",
      " \n",
      "Latente semantische Analyse LSA mit Themenanzahl  4\n",
      " \n",
      "Reviews:\n",
      "Topic  0  :  0.014616169029097442\n",
      "Topic  1  :  -0.0061677133361215134\n",
      "Topic  2  :  -0.0010259566482233612\n",
      "Topic  3  :  -0.007290577168824961\n",
      " \n",
      "Latente semantische Analyse LSA mit Themenanzahl  5\n",
      " \n",
      "Reviews:\n",
      "Topic  0  :  0.014614969333785211\n",
      "Topic  1  :  -0.00622500899354343\n",
      "Topic  2  :  -0.0010333573981407814\n",
      "Topic  3  :  -0.007408891037162962\n",
      "Topic  4  :  -0.004603706167514176\n",
      " \n",
      "Latente semantische Analyse LSA mit Themenanzahl  6\n",
      " \n",
      "Reviews:\n",
      "Topic  0  :  0.01461581672353183\n",
      "Topic  1  :  -0.0061792979523492934\n",
      "Topic  2  :  -0.0010904087505537816\n",
      "Topic  3  :  -0.007090442230465507\n",
      "Topic  4  :  -0.004381741909381594\n",
      "Topic  5  :  -0.00029158371778988503\n",
      " \n",
      "Latente semantische Analyse LSA mit Themenanzahl  7\n",
      " \n",
      "Reviews:\n",
      "Topic  0  :  0.01461611468078479\n",
      "Topic  1  :  -0.006149721815305496\n",
      "Topic  2  :  -0.0010523095022717227\n",
      "Topic  3  :  -0.007323147878180101\n",
      "Topic  4  :  -0.004624111734921788\n",
      "Topic  5  :  -0.0002085399019087881\n",
      "Topic  6  :  0.0075247430032003276\n",
      " \n",
      "Latente Dirichlet Allocation LDA mit Themenanzahl  2\n",
      " \n",
      "Reviews: \n",
      "Topic  0  ID   :  0.8065627050070449\n",
      "Topic  1  ID   :  0.19343729499295512\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 2)\n",
      "Topic by word matrix:  (2, 13600)\n",
      " \n",
      "Latente Dirichlet Allocation LDA mit Themenanzahl  3\n",
      " \n",
      "Reviews: \n",
      "Topic  0  ID   :  0.7726238806013926\n",
      "Topic  1  ID   :  0.11239117204703261\n",
      "Topic  2  ID   :  0.11498494735157491\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 3)\n",
      "Topic by word matrix:  (3, 13600)\n",
      " \n",
      "Latente Dirichlet Allocation LDA mit Themenanzahl  4\n",
      " \n",
      "Reviews: \n",
      "Topic  0  ID   :  0.7468105924442547\n",
      "Topic  1  ID   :  0.08382923592912132\n",
      "Topic  2  ID   :  0.08550941264971636\n",
      "Topic  3  ID   :  0.08385075897690758\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 4)\n",
      "Topic by word matrix:  (4, 13600)\n",
      " \n",
      "Latente Dirichlet Allocation LDA mit Themenanzahl  5\n",
      " \n",
      "Reviews: \n",
      "Topic  0  ID   :  0.7317961348468648\n",
      "Topic  1  ID   :  0.06704035077427764\n",
      "Topic  2  ID   :  0.06708394591272067\n",
      "Topic  3  ID   :  0.06704035961453006\n",
      "Topic  4  ID   :  0.0670392088516068\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 5)\n",
      "Topic by word matrix:  (5, 13600)\n",
      " \n",
      "Latente Dirichlet Allocation LDA mit Themenanzahl  6\n",
      " \n",
      "Reviews: \n",
      "Topic  0  ID   :  0.055863666376530806\n",
      "Topic  1  ID   :  0.05586369754669658\n",
      "Topic  2  ID   :  0.05613267197609943\n",
      "Topic  3  ID   :  0.05586376607535986\n",
      "Topic  4  ID   :  0.7204125256199008\n",
      "Topic  5  ID   :  0.0558636724054125\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 6)\n",
      "Topic by word matrix:  (6, 13600)\n",
      " \n",
      "Latente Dirichlet Allocation LDA mit Themenanzahl  7\n",
      " \n",
      "Reviews: \n",
      "Topic  0  ID   :  0.04788273940272878\n",
      "Topic  1  ID   :  0.047882746158381014\n",
      "Topic  2  ID   :  0.04788273318956721\n",
      "Topic  3  ID   :  0.04788271864092899\n",
      "Topic  4  ID   :  0.04788272517419735\n",
      "Topic  5  ID   :  0.04788274513692703\n",
      "Topic  6  ID   :  0.7127035922972697\n",
      " \n",
      " \n",
      "Documents by topic matrix:  (17703, 7)\n",
      "Topic by word matrix:  (7, 13600)\n",
      " \n",
      "Coherence topic_number= 2  result  -14.673842117141106\n",
      "Coherence topic_number= 3  result  -14.674866018929256\n",
      "Coherence topic_number= 4  result  -14.681521380552244\n",
      "Coherence topic_number= 5  result  -14.678142504651342\n",
      "Coherence topic_number= 6  result  -14.693296251115989\n",
      "Coherence topic_number= 7  result  -14.685690123546864\n"
     ]
    }
   ],
   "source": [
    "def calculateCoherenceScore(model):\n",
    "    '''Implementierung des Coherence Score für die Ermittlung der optimalen Anzahl an Topics'''\n",
    "    '''Tests jeweils mit 2 bis 8 Topics'''\n",
    "    for x in range(2,8):\n",
    "        model = LdaModel(common_corpus, num_topics=x) # wrong model only for texting\n",
    "        c_model=CoherenceModel(model=model, corpus=common_corpus, dictionary=common_dictionary, coherence='u_mass')\n",
    "        coherence = c_model.get_coherence()\n",
    "        print(\"Coherence topic_number=\",x,\" result \",coherence)\n",
    "\n",
    "def calculateLSA(model):\n",
    "    '''Implementierung der LSA-Technik der semantischen Analyse'''\n",
    "    '''Tests jeweils mit 2 bis 8 Topics'''\n",
    "    for x in range(2,8):\n",
    "        lsa_model = TruncatedSVD(n_components=x,algorithm='randomized',n_iter=10)\n",
    "        lsa = lsa_model.fit_transform(model)\n",
    "        l=lsa[0]\n",
    "        '''Zwischenausgabe der LSA Daten'''\n",
    "        print(\"Latente semantische Analyse LSA mit Themenanzahl \",x)\n",
    "        print(\" \")\n",
    "        print(\"Reviews:\")\n",
    "        for i,topic in enumerate(l):\n",
    "            print(\"Topic \",i,\" : \", topic)\n",
    "        print(\" \")\n",
    "\n",
    "def calculateLDA(model):\n",
    "    '''Implementierung der LDA-Technik der semantischen Analyse'''\n",
    "    '''Tests jeweils mit 2 bis 8 Topics'''\n",
    "    lda_model=LatentDirichletAllocation(n_components=2,learning_method='online',random_state=42,max_iter=1)\n",
    "    for x in range(2,8):\n",
    "        lda_model=LatentDirichletAllocation(n_components=x,learning_method='online',random_state=42,max_iter=1)\n",
    "        lda_top=lda_model.fit_transform(model)\n",
    "        '''Zwischenausgabe der LDA Daten'''\n",
    "        print(\"Latente Dirichlet Allocation LDA mit Themenanzahl \",x)\n",
    "        print(\" \")\n",
    "        print(\"Reviews: \")\n",
    "        for i,topic in enumerate(lda_top[0]):\n",
    "            print(\"Topic \",i,\" ID \",\" : \",topic)\n",
    "        print(\" \")\n",
    "        print(\" \")\n",
    "        print(\"Documents by topic matrix: \",lda_top.shape)\n",
    "        print(\"Topic by word matrix: \",lda_model.components_.shape)\n",
    "        print(\" \")\n",
    "    return lda_model\n",
    "\n",
    "def printNLPdata():\n",
    "    print(\"Method Test\")\n",
    "\n",
    "\n",
    "def plotNLPdata():\n",
    "    print(\"Method Test\")\n",
    "    \n",
    "calculateLSA(model)\n",
    "model = calculateLDA(model)\n",
    "calculateCoherenceScore(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6d00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
