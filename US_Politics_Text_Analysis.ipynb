{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9430fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "'''Import der Bibliotheken'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import gensim.corpora as corpora\n",
    "# For tokenization - ToDo maybe not necessary\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('wordnet')\n",
    "# For lemmatization - ToDo maybe not necessary\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "# For stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb25f950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    title  score  \\\n",
      "id                                                                 \n",
      "ov1ll3  A Right Wing Group in Texas Is Making up Fake ...    166   \n",
      "ouwc9i  DOJ sues Texas over Gov. Abbott’s order for la...     85   \n",
      "ouqkxi  From white evangelicals to QAnon believers, wh...     57   \n",
      "oun2lc  DeSantis says he’ll sign order allowing parent...    269   \n",
      "ouipnz  Show on the road: In Utah, Florida Gov. Ron De...     31   \n",
      "ou0w7e  Michigan Supreme Court limits use of restraint...    103   \n",
      "otzggh  'Election integrity committee' in York County ...    261   \n",
      "otzb3p  Texas Senator Used MLK’s Words To Attack Criti...    136   \n",
      "osvxyl  Wisconsin GOP leader doesn't want another elec...    105   \n",
      "osvwbq       Democrats press Biden to extend eviction ban     54   \n",
      "\n",
      "                                                      url  comms_num  \\\n",
      "id                                                                     \n",
      "ov1ll3  https://www.vice.com/en/article/wx5bg5/blm-whi...         34   \n",
      "ouwc9i  https://www.kxan.com/news/texas-politics/doj-s...         17   \n",
      "ouqkxi  https://www.modbee.com/news/coronavirus/articl...         27   \n",
      "oun2lc  https://www.orlandosentinel.com/politics/os-ne...        138   \n",
      "ouipnz  https://www.tallahassee.com/story/news/politic...         28   \n",
      "ou0w7e  https://www.metrotimes.com/news-hits/archives/...          2   \n",
      "otzggh  https://www.yorkdispatch.com/story/news/local/...         20   \n",
      "otzb3p  https://www.keranews.org/politics/2021-07-29/t...         18   \n",
      "osvxyl  https://abcnews.go.com/Politics/wireStory/wisc...         10   \n",
      "osvwbq  https://www.politico.com/news/2021/07/27/democ...         29   \n",
      "\n",
      "             created body            timestamp  \n",
      "id                                              \n",
      "ov1ll3  1.627710e+09  NaN  2021-07-31 08:35:47  \n",
      "ouwc9i  1.627688e+09  NaN  2021-07-31 02:26:12  \n",
      "ouqkxi  1.627671e+09  NaN  2021-07-30 21:45:09  \n",
      "oun2lc  1.627660e+09  NaN  2021-07-30 18:43:05  \n",
      "ouipnz  1.627644e+09  NaN  2021-07-30 14:21:54  \n",
      "ou0w7e  1.627576e+09  NaN  2021-07-29 19:30:52  \n",
      "otzggh  1.627572e+09  NaN  2021-07-29 18:16:28  \n",
      "otzb3p  1.627571e+09  NaN  2021-07-29 18:08:34  \n",
      "osvxyl  1.627422e+09  NaN  2021-07-28 00:43:18  \n",
      "osvwbq  1.627422e+09  NaN  2021-07-28 00:40:50  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Numbers of rows in Dataframe:  28063\n"
     ]
    }
   ],
   "source": [
    "'''Method for reading data from csv and save as type DataFrame (Pandas)'''\n",
    "'''index column in this special data set is the third column'''\n",
    "def inputData(url):\n",
    "    input_data_csv = pd.read_csv(url,index_col=2)\n",
    "    return input_data_csv\n",
    "    \n",
    "'''Data Input as .csv from github'''\n",
    "'''@param: ?raw=true in url important for using clean original data'''\n",
    "data_url = 'https://github.com/freezz88/US_Politics_Text_Analysis/blob/main/reddit_politics.csv?raw=true'\n",
    "data = inputData(data_url)\n",
    "print(data.head(10))\n",
    "print(type(data))\n",
    "print(\"Number of rows in DataFrame: \", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc34b24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in DataFrame after Cleaning:  17731\n",
      "Sentences after filtering the dataset:\n",
      "0        I had the same reasoning when I watch fox news...\n",
      "1             Unethical fucks will always find a loophole.\n",
      "2                                      Failed actual coup.\n",
      "3                   Why is trump even in the news anymore?\n",
      "4                   And it could be my head in a basket...\n",
      "                               ...                        \n",
      "17726            lil'wayne got a pardon and not them ah ah\n",
      "17727    So you think it will be called unconstitutiona...\n",
      "17728    The left of America has out numbered the right...\n",
      "17729    Everyone spread the word…I just set fire on water\n",
      "17730    Starting to feel like congress should let DOJ ...\n",
      "Name: body, Length: 17703, dtype: object\n",
      "<class 'pandas.core.series.Series'>\n",
      " \n",
      "Sentences after Stemming:\n",
      "['0', '       ', 'i', 'had', 'the', 'same', 'reason', 'when', 'i', 'watch', 'fox', 'news', '...', '\\n', '1', '            ', 'uneth', 'fuck', 'will', 'alway', 'find', 'a', 'loophol', '.', '\\n', '2', '                                     ', 'fail', 'actual', 'coup', '.', '\\n', '3', '                  ', 'whi', 'is', 'trump', 'even', 'in', 'the', 'news', 'anymor', '?', '\\n', '4', '                  ', 'and', 'it', 'could', 'be', 'my', 'head', 'in', 'a', 'basket', '...', '\\n                               ', '...', '                       \\n', '17726', '           ', \"lil'wayn\", 'got', 'a', 'pardon', 'and', 'not', 'them', 'ah', 'ah', '\\n', '17727', '   ', 'so', 'you', 'think', 'it', 'will', 'be', 'call', 'unconstitutiona', '...', '\\n', '17728', '   ', 'the', 'left', 'of', 'america', 'ha', 'out', 'number', 'the', 'right', '...', '\\n', '17729', '   ', 'everyon', 'spread', 'the', 'word', '…', 'i', 'just', 'set', 'fire', 'on', 'water', '\\n', '17730', '   ', 'start', 'to', 'feel', 'like', 'congress', 'should', 'let', 'doj', '...', '\\n', 'name', ':', 'bodi', ',', 'length', ':', '17703', ',', 'dtype', ':', 'object']\n",
      "<class 'list'>\n",
      " \n",
      "Sentences after Text Preprocessing:\n",
      "0  i had the same reasoning when i watch fox news..\n",
      "1  unethical fucks will always find a loophole.\n",
      "2  failed actual coup.\n",
      "3  why is trump even in the news anymore?\n",
      "4  and it could be my head in a basket..\n",
      "  ..  \n",
      "17726  lilwayne got a pardon and not them ah ah\n",
      "17727  so you think it will be called unconstitutiona..\n",
      "17728  the left of america has out numbered the right..\n",
      "17729  everyone spread the wordi just set fire on water\n",
      "17730  starting to feel like congress should let doj ..\n",
      "name body, length 17703, dtype object\n",
      "<class 'str'>\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\freez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "'''Cleaning data'''\n",
    "# delete duplicate reviews - column body\n",
    "data.drop_duplicates(subset='body', inplace=True)\n",
    "# delete reviews without text\n",
    "data.dropna(subset=['body'], inplace=True)\n",
    "# Reset the index after the deletion of rows\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "print(\"Number of rows in DataFrame after Cleaning: \",len(data))\n",
    "\n",
    "'''Filter text for the category comments'''\n",
    "'''Only show the column body, the others doesnt matter'''\n",
    "data_reviews = data['title'] == \"Comment\"\n",
    "filtered_data_list = data[data_reviews]\n",
    "#filtered_dataframe = pd.DataFrame(filtered_data_list)\n",
    "#reviews = filtered_dataframe['body']\n",
    "reviews = filtered_data_list['body']\n",
    "reviews_string = str(reviews)\n",
    "print(\"Sentences after filtering the dataset:\")\n",
    "print(reviews)\n",
    "print(type(reviews))\n",
    "print(\" \")\n",
    "\n",
    "'''Text Preprocessing'''\n",
    "'''I Tokenization - ToDo check later for optimization '''\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Tokenize the text\n",
    "doc = nlp(reviews_string)\n",
    "# Extract tokens\n",
    "tokens = [token.text for token in doc]\n",
    "#print(tokens)\n",
    "'''II Download and definition of stopwords with NLTK - ToDo append new stopwords'''\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "'''III Stemming / Lemmatization - ToDo check later for optimization'''\n",
    "\n",
    "# For Stemming\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# Stemming each token\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Sentences after Stemming:\")\n",
    "print(stemmed_tokens)\n",
    "print(type(stemmed_tokens))\n",
    "print(\" \")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  \n",
    "    # Remove special characters, keeping only words and basic charakters\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s,.?!]', '', text)  \n",
    "    # Reduce massive character repetition to a maximum of two charakters\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)   \n",
    "    return text\n",
    "\n",
    "changed_data = preprocess_text(reviews_string)\n",
    "print(\"Sentences after Text Preprocessing:\")\n",
    "print(changed_data)\n",
    "print(type(changed_data))\n",
    "print(\" \")\n",
    "dict_data = {changed_data.index : changed_data} # ToDo index has to be separate saved from dataframe\n",
    "#print(dict_data)\n",
    "#print(type(dict_data))\n",
    "#print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c34fec5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW-Modell Daten\n",
      " \n",
      "       00  000  0000017a  000557  000s  001  00220  004303  005  00818  ...  \\\n",
      "0       0    0         0       0     0    0      0       0    0      0  ...   \n",
      "1       0    0         0       0     0    0      0       0    0      0  ...   \n",
      "2       0    0         0       0     0    0      0       0    0      0  ...   \n",
      "3       0    0         0       0     0    0      0       0    0      0  ...   \n",
      "4       0    0         0       0     0    0      0       0    0      0  ...   \n",
      "...    ..  ...       ...     ...   ...  ...    ...     ...  ...    ...  ...   \n",
      "17698   0    0         0       0     0    0      0       0    0      0  ...   \n",
      "17699   0    0         0       0     0    0      0       0    0      0  ...   \n",
      "17700   0    0         0       0     0    0      0       0    0      0  ...   \n",
      "17701   0    0         0       0     0    0      0       0    0      0  ...   \n",
      "17702   0    0         0       0     0    0      0       0    0      0  ...   \n",
      "\n",
      "       zwaan  zygote  zzz  áñez  état  ˈmeɪnstriːm  χρ  χριστος  ꜱimp  ꜱo  \n",
      "0          0       0    0     0     0            0   0        0     0   0  \n",
      "1          0       0    0     0     0            0   0        0     0   0  \n",
      "2          0       0    0     0     0            0   0        0     0   0  \n",
      "3          0       0    0     0     0            0   0        0     0   0  \n",
      "4          0       0    0     0     0            0   0        0     0   0  \n",
      "...      ...     ...  ...   ...   ...          ...  ..      ...   ...  ..  \n",
      "17698      0       0    0     0     0            0   0        0     0   0  \n",
      "17699      0       0    0     0     0            0   0        0     0   0  \n",
      "17700      0       0    0     0     0            0   0        0     0   0  \n",
      "17701      0       0    0     0     0            0   0        0     0   0  \n",
      "17702      0       0    0     0     0            0   0        0     0   0  \n",
      "\n",
      "[17703 rows x 25480 columns]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "'''Implementation Bag-of-words'''\n",
    "'''ToDo wrong data - the data from text preprocessing not used'''\n",
    "def calculateBoW():\n",
    "    vect = CountVectorizer(stop_words=stop_words_english)\n",
    "    #data = vect.fit_transform([reviews_string])\n",
    "    bow_data = vect.fit_transform(reviews)\n",
    "    bow_data = pd.DataFrame(bow_data.toarray(),columns=vect.get_feature_names())\n",
    "    '''Zwischenausgabe der Bow-Modell Daten'''\n",
    "    print(\"BoW-Modell Daten\")\n",
    "    print(\" \")\n",
    "    print(bow_data)\n",
    "    print(\" \")\n",
    "    \n",
    "calculateBoW()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf94fe5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-idf Daten Reviews\n",
      " \n",
      "        00  000  0000017a  000557  000s  001  00220  004303  005  00818  ...  \\\n",
      "0      0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "1      0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "2      0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "3      0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "4      0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "...    ...  ...       ...     ...   ...  ...    ...     ...  ...    ...  ...   \n",
      "17698  0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "17699  0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "17700  0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "17701  0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "17702  0.0  0.0       0.0     0.0   0.0  0.0    0.0     0.0  0.0    0.0  ...   \n",
      "\n",
      "       zwaan  zygote  zzz  áñez  état  ˈmeɪnstriːm   χρ  χριστος  ꜱimp   ꜱo  \n",
      "0        0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "1        0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "2        0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "3        0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "4        0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "...      ...     ...  ...   ...   ...          ...  ...      ...   ...  ...  \n",
      "17698    0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "17699    0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "17700    0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "17701    0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "17702    0.0     0.0  0.0   0.0   0.0          0.0  0.0      0.0   0.0  0.0  \n",
      "\n",
      "[17703 rows x 25480 columns]\n",
      " \n"
     ]
    }
   ],
   "source": [
    "'''Implementation Tf-idf'''\n",
    "'''ToDo wrong data - the data from text preprocessing not used'''\n",
    "def calculateTfidf():\n",
    "    #vectorizer = TfidfVectorizer(min_df=1) first version TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(use_idf=True,\n",
    "    smooth_idf=True, stop_words=stop_words_english)\n",
    "    #model = vectorizer.fit_transform([reviews_string]) wrong usage, wrong datatype\n",
    "    model = vectorizer.fit_transform(reviews)\n",
    "    data2=pd.DataFrame(model.toarray(),columns=vectorizer.get_feature_names())\n",
    "    '''Zwischenausgabe der TF-idf Daten'''\n",
    "    print(\"TF-idf Daten Reviews\")\n",
    "    print(\" \")\n",
    "    print(data2)\n",
    "    print(\" \")\n",
    "    \n",
    "calculateTfidf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e076930",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Implementation in near future'''\n",
    "def calculateCoherenceScore():\n",
    "    print(\"Method Test\")\n",
    "\n",
    "def calculateLSA():\n",
    "    print(\"Method Test\")\n",
    "\n",
    "def calculateLDA():\n",
    "    print(\"Method Test\")\n",
    "\n",
    "def printNLPdata():\n",
    "    print(\"Method Test\")\n",
    "\n",
    "def plotNLPdata():\n",
    "    print(\"Method Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c6d00a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
